from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer, BertModel, pipeline
from pymongo import MongoClient
from PyPDF2 import PdfReader
from PIL import Image
from flask_socketio import SocketIO, emit
import os
import datetime
import time
import json
import uuid
import faiss
import fitz
import pickle
import docx
import re
import numpy as np
import torch
import pytesseract
import io
import threading

socketio = SocketIO()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_name ="model_trained/wangchanberta"
# Load Sentence Transformer model for embeddings
embedder = SentenceTransformer(model_name)
#tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Initialize FAISS index
d = 768  # Embedding dimension
#faiss_index = faiss.IndexFlatL2(d)
faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(d))

vector_store = []  # Store metadata

def extract_text_from_pdf(pdf_path, file_id):
    try:
        doc = fitz.open(pdf_path)
        text_list = []
        for page in doc:
            text = page.get_text("text").encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")
            text_list.append(text)
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not any(text.strip() for text in text_list):
            print(f"‚ùå No text found in PDF {pdf_path}, trying OCR method...")
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°, ‡πÉ‡∏ä‡πâ OCR
            text_list = extract_text_ocr_pdf(pdf_path, file_id)
        
        return text_list
    except Exception as e:
        print(f"‚ùå Error extracting text from PDF {pdf_path}: {e}")
        return []

def extract_text_ocr_pdf(pdf_path, file_id):
    try:
        doc = fitz.open(pdf_path)
        text_list = []
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤ PDF ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
            pix = page.get_pixmap()
            img = Image.open(io.BytesIO(pix.tobytes("png")))  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û PNG

            # ‡πÉ‡∏ä‡πâ OCR ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            text_from_image = pytesseract.image_to_string(img)

            # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ OCR ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if text_from_image.strip() == "":
                print(f"‚ùå No text found by OCR on page {page_num + 1} in PDF {pdf_path}")
                text_list.append("No text found by OCR")
            else:
                text_list.append(text_from_image)
        
        return text_list
    except Exception as e:
        print(f"‚ùå Error extracting text from PDF {pdf_path}: {e}")
        return []

def extract_text_from_txt(file_path, file_id):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    return [text]

def extract_text_from_json(file_path, file_id):
    import json
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
    return [json.dumps(data)]

def extract_text_from_docx(file_path, file_id):
    from docx import Document
    doc = Document(file_path)
    text = ""
    for para in doc.paragraphs:
        text += para.text + '\n'
    return [text]

def get_tokenizer(model_name):
    """‡∏î‡∏∂‡∏á tokenizer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ thread"""
    if not hasattr(thread_local, "tokenizer"):
        thread_local.tokenizer = AutoTokenizer.from_pretrained(model_name)
    return thread_local.tokenizer

def split_text(text, chunk_size=256):
    tokenizer = get_tokenizer(model_name)
    tokens = tokenizer.tokenize(text)
    chunks = [tokenizer.convert_tokens_to_string(tokens[i:i+chunk_size]) for i in range(0, len(tokens), chunk_size)]
    return chunks

def preprocess_query(query):
    query = query.lower().strip()
    query = re.sub(r"[^a-zA-Z0-9‡∏Å-‡πô\s]", "", query)
    return query


read_files = []

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô BERT embeddings
def extract_bert_embeddings(text):
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô token IDs ‡∏ó‡∏µ‡πà BERT ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
    if isinstance(text, list):  
        text = " ".join(text) 
    tokenized = get_tokenizer(model_name)
    inputs = tokenized(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        output = model(**inputs)
    #print(f"‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö : {output}")
    #embeddings = output.last_hidden_state[:, 0, :].numpy()
    #return embeddings
    embeddings = output.last_hidden_state.mean(dim=1)  # ‡πÉ‡∏ä‡πâ mean pooling
    return embeddings.numpy()

file_id = str(uuid.uuid4())

def get_next_file_number(output_path):
    """ ‡∏´‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏à‡∏≤‡∏Å vector_store """
    existing_files = [f for f in os.listdir(output_path) if f.endswith('.faiss')]
    file_numbers = [int(f.split('_')[1].split('.')[0]) for f in existing_files if f.startswith('file_') and f.split('_')[1].split('.')[0].isdigit()]
    return max(file_numbers, default=0) + 1



def insert_metadata(role, file_name, file_type, file_id, input_file_name): 
    try:
        connectionname = 'vectors_' + role 
        client = MongoClient('mongodb://localhost:27017')
        db = client['rag_db']
        collection = db[connectionname]


        metadata = {
            "file_name": file_name, 
            "file_type": file_type, 
            "file_id": file_id, 
            "input_file_name": input_file_name, 
            "created_at": datetime.datetime.utcnow() 
        }

        result = collection.update_one(
            {"file_id": file_id},
            {"$set": metadata},
            upsert=True
        )

        if result.upserted_id:
            print(f"New metadata inserted for file {file_name}")
        else:
            print(f"Metadata for file {file_name} already exists.")


    except Exception as e:
        print(f"Error inserting metadata into MongoDB: {e}")

def search_faiss_in_folder(query_text, faiss_folder, top_k=3):
    query_embedding = extract_bert_embeddings([query_text])
    #print(f"üîé Query: {query_text}")
    #print("Query embedding shape before reshape:", query_embedding.shape)

    # ‚úÖ Reshape ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    if len(query_embedding.shape) == 3:
        query_embedding = query_embedding.reshape(query_embedding.shape[1:])
    elif len(query_embedding.shape) == 1:
        query_embedding = query_embedding.reshape(1, -1)

    #print("Query embedding shape after reshape:", query_embedding.shape)
    #print(f"üîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Query embedding: {query_embedding[:5]}")
    faiss_files = [f for f in os.listdir(faiss_folder) if f.endswith('.faiss')]
    pickle_files = [f for f in os.listdir(faiss_folder) if f.endswith('.pkl')]
    all_distances = []
    all_indices = []
    retrieved_texts = []
    metadata = []

    for faiss_file, pickle_file in zip(faiss_files, pickle_files):
        index_path = os.path.join(faiss_folder, faiss_file)
        pickle_path = os.path.join(faiss_folder, pickle_file)

        # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î FAISS index
        index = faiss.read_index(index_path)

        # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î pickle
        with open(pickle_path, "rb") as f:
            text_list = pickle.load(f)

        #print(f"üìÇ Searching in {faiss_file} (dimension: {index.d})")

        # ‚ùå ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if query_embedding.shape[1] != index.d:
            #raise ValueError(f"Dimension mismatch: query embedding has {query_embedding.shape[1]} dimensions, "
            #                 f"but index expects {index.d} dimensions.")
            print(f"‚ö†Ô∏è Skipping {faiss_file}: Dimension mismatch ({query_embedding.shape[1]} vs {index.d})")
            continue  
        
        # ‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        distances, indices = index.search(query_embedding, min(3, index.ntotal))
        #print(f"üéØ ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤: indices = {indices}, distances = {distances}")
        #print(f"üéØ FAISS Results ({faiss_file}):")
        """for idx in (indices[0]):
            if idx >= 0 and idx < len(text_list):
                retrieved_texts.append(text_list[idx]) # text_list to text_list_split
                metadata.append({
                    "file_name": pickle_file,
                    "distance": distances[0],
                    "text": text_list[idx]
                })"""
        for i in range(len(indices[0])):
            idx = indices[0][i]
            if idx >=0 and idx < len(text_list):
                retrieved_texts.append(text_list[idx])
                metadata.append({
                    "file_name": pickle_file,
                    "faiss_file": faiss_file,
                    "distance": distances[0][i],
                    "text": text_list[idx]
                })
                all_distances.append(distances[0][i])
                all_indices.append(idx)


    #print(f"üîé Retrieved Texts: {retrieved_texts}")
    if not retrieved_texts:
        print("üö´ No relevant documents found.")
    return distances, indices, retrieved_texts, metadata

thread_local = threading.local()

def process_file(role, file_path, output_path, file_id, socketio):
    print(f"Processing {file_path}")
    
    if file_id not in STATUS:
        STATUS[file_id] = {"progress": 0}
    for i in range(1, 11):
        socketio.sleep(1) 
        STATUS[file_id]["progress"] = i * 10
               
        print(f"üîÑ Sent progress update: {STATUS[file_id]['progress']}% for {file_id}")

        if i < 2:
            STATUS[file_id]["progress"] = 20
            file_extension = os.path.splitext(file_path)[1].lower()
            STATUS[file_id]["detailed_message"] = "üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå..."
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
            if file_extension == '.txt':
                text_list = extract_text_from_txt(file_path, file_id)
            elif file_extension == '.json':
                text_list = extract_text_from_json(file_path, file_id)
            elif file_extension == '.docx':
                text_list = extract_text_from_docx(file_path, file_id)
            elif file_extension == '.pdf':
                text_list = extract_text_from_pdf(file_path, file_id)
            else:
                return None
            print(f"üìú text_list ({len(text_list)} items): {text_list[:5]}")

            text_list_split = []
            for text in text_list:
                text_list_split.extend(split_text(text))  # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô chunks
                embeddings = extract_bert_embeddings(text_list_split)

            print(f"üîπ text_list_split ({len(text_list_split)} items): {text_list_split[:5]}")
            
            # ‚úÖ ‡πÉ‡∏ä‡πâ text_list ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
            """if text_list is not None:
                embeddings = extract_bert_embeddings(text_list)
                print("Before reshape, embeddings shape:", embeddings.shape)"""

        elif i < 4 :
            STATUS[file_id]["progress"] = 40
            STATUS[file_id]["detailed_message"] = "‚úçüèª ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Vector ‡πÅ‡∏•‡∏∞ Metadata"
            # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô (N, 768)
            if len(embeddings.shape) == 3:
                embeddings = embeddings.squeeze(axis=1)

            print("After reshape, embeddings shape:", embeddings.shape)

            if embeddings.shape[1] != 768:
                raise ValueError(f"Expected embedding shape (N, 768), but got {embeddings.shape}")

            # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS index
            dimension = embeddings.shape[1]
            index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))
            # ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á ID ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
            ids = np.arange(embeddings.shape[0]).astype(np.int64)  # ID ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô int64

            # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏•‡∏á‡πÉ‡∏ô FAISS index ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ add_with_ids
            index.add_with_ids(embeddings, ids)
            print(f"‚úÖ Added {embeddings.shape[0]} vectors to FAISS index with IDs")
            
            # ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
            file_number = get_next_file_number(output_path)
            index_filename = os.path.join(output_path, f'file_{file_number}.faiss')
            pickle_filename = os.path.join(output_path, f'file_{file_number}.pkl')        
        elif i == 6:
            STATUS[file_id]["progress"] = 60
            STATUS[file_id]["detailed_message"] = "üìÇ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å FAISS..."
            # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å FAISS index
            try:
                faiss.write_index(index, index_filename)
            except Exception as e:
                print(f"Error saving FAISS index: {e}")
                return None
        elif i == 8:
            STATUS[file_id]["progress"] = 80
            STATUS[file_id]["detailed_message"] = "üîÑ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Pickle..."
            # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            try:
                with open(pickle_filename, 'wb') as f:
                    pickle.dump(text_list, f)
            except Exception as e:
                print(f"Error saving pickle file: {e}")
                return None
        elif i == 10:
            STATUS[file_id]["progress"] = 100 
            STATUS[file_id]["detailed_message"] = "‚úÖ Upload complete!"
            insert_metadata(role, index_filename, "faiss", file_number, os.path.basename(file_path))

            print(f"FAISS index and pickle file saved as {index_filename} and {pickle_filename}")
       
        socketio.emit("upload_status", {"file_id": file_id, 
                                        "progress": STATUS[file_id]["progress"],
                                        "file_name": os.path.basename(file_path)})

        socketio.sleep(1)
    return text_list

STATUS = {"progress": 0, "message": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î"}
thread_local.STATUS = {"progress": 0, "message": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î"}

def upload_progress(file_id, socket_io):
    global STATUS

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ file_id ‡πÉ‡∏ô STATUS ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á
    if file_id not in STATUS:
        STATUS[file_id] = {"progress": 0, "message": "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î..."}
    for i in range(1, 11):
        print("‡∏Ñ‡πà‡∏≤  I : ", i)
        time.sleep(1)
        STATUS[file_id]["progress"] = i * 10 
        if i < 2:
            STATUS[file_id]["progress"] = 20
            STATUS[file_id]["detailed_message"] = "üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå..."
        elif i < 4:
            STATUS[file_id]["progress"] = 40
            STATUS[file_id]["detailed_message"] = "‚úçüèª ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Vector ‡πÅ‡∏•‡∏∞ Metadata"
        elif i < 6:
            STATUS[file_id]["progress"] = 60
            STATUS[file_id]["detailed_message"] = "üìÇ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å FAISS..."
        elif i < 8:
            STATUS[file_id]["progress"] = 80
            STATUS[file_id]["detailed_message"] = "üîÑ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Pickle..."
        elif i >= 10:
            STATUS[file_id]["progress"] = 100
            STATUS[file_id]["detailed_message"] = "‚úÖ Upload complete!"
            
        
        # Emit status to the client
        socket_io.emit('upload_status', {"file_id": file_id, **STATUS[file_id]}, broadcast=True)
   
def check_new_files(input_path, output_path, file_path, file_id):
    global read_files
    for filename in os.listdir(input_path):
        file_path = os.path.join(input_path, filename)
        
        if file_path not in read_files:
            process_file(file_path, output_path, file_id, socketio)
            read_files.append(file_path)

if __name__ == "__main__":
    file_id = str(uuid.uuid4())
    check_new_files()
    time.sleep(300)

"""
def loadtyphoon2(modelconfig):
    load_path = os.path.join("model_trained", modelconfig)

    typhoon2_model = AutoModelForCausalLM.from_pretrained(
        load_path,
        device_map="auto",
        local_files_only=True
    )
    typhoon2_tokenizer = AutoTokenizer.from_pretrained(load_path, local_files_only=True)
    return typhoon2_model, typhoon2_tokenizer


## Russian  Tonkey : ‡∏Ñ‡∏¥‡∏î‡∏î‡∏π‡∏™‡∏¥
def typhoon2chat(query, typhoon2_model, typhoon2_tokenizer):
    messages = [
        {"role": "system", "content": "You are a male AI assistant named Tonkey created by TRU to be helpful, harmless, and honest. Tonkey is happy to help with analysis, question answering, math, coding, creative writing, teaching, role-play, general discussion, and all sorts of other tasks. Tonkey responds directly to all human messages without unnecessary affirmations or filler phrases like ‚ÄúCertainly!‚Äù, ‚ÄúOf course!‚Äù, ‚ÄúAbsolutely!‚Äù, ‚ÄúGreat!‚Äù, ‚ÄúSure!‚Äù, etc. Specifically, Tonkey avoids starting responses with the word ‚ÄúCertainly‚Äù in any way. Tonkey follows this information in all languages, and always responds to the user in the language they use or request. Tonkey is now being connected with a human. Write in fluid, conversational prose, Show genuine interest in understanding requests, Express appropriate emotions and empathy. Also showing information in term that is easy to understand and visualized."},
        {"role": "user", "content": query},
    ]

    input_ids = typhoon2_tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    terminators = [
        typhoon2_tokenizer.eos_token_id,
        typhoon2_tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    outputs = typhoon2_model.generate(
        input_ids,
        max_new_tokens=512,
        eos_token_id=terminators,
        do_sample=True,
        temperature=0.4,
        top_p=0.9,
    )

    response = outputs[0][input_ids.shape[-1]:]
    return typhoon2_tokenizer.decode(response, skip_special_tokens=True)


 
if __name__ == "__main__":
    file_id = str(uuid.uuid4())
    check_new_files()
    time.sleep(300)


if __name__ == "__main__":
    print("Let's chat! (type 'quit' to exit)")
    while True:
        fdpath = "documents/json_main"
        faiss_folder="vector_store"
        sentence = input("You: ")
        if sentence.lower() == "quit":
            break      
        print("AI is generating a response...")
        distances_all, indices_all, retrieved_texts, metadata = search_faiss_in_folder(sentence, faiss_folder, top_k=3)
        print(f"Retrieved Text lastest: {retrieved_texts}")
        print(f"Metadata : {metadata}")
        #for i, text in enumerate(retrieved_texts):
        #    print(f"Item {i}: {type(text)}")
        
        context = " ".join(str(text) for text in retrieved_texts)
        print("context : ", context)                          
        input_text = f"‡∏ö‡∏£‡∏¥‡∏ö‡∏ó: {context} \n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {sentence}"

        model, tokenizer = loadtyphoon2("local_typhoon2_1b")
        response = typhoon2chat(input_text, model, tokenizer)
        print(f"answer : {response}")
"""

# ‡πÄ‡∏≠‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏î‡∏ö‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ
# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏û‡∏≤‡∏Å‡∏©‡πå‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏®‡∏¥‡∏•‡∏õ‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô
# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏®‡∏¥‡∏•‡∏õ‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô
# ‡πÄ‡∏≠‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ ‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ‡∏ó‡∏µ‡πà2 ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á
# ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏£‡∏±‡∏ê‡∏õ‡∏£‡∏∞‡∏®‡∏≤‡∏™‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏£‡∏±‡∏ê‡∏õ‡∏£‡∏∞‡∏®‡∏≤‡∏™‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏Ñ‡∏£‡∏±‡∏ö
# ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏£‡∏±‡∏ö
# ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤ ‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏£‡∏±‡∏ê‡∏õ‡∏£‡∏∞‡∏®‡∏≤‡∏™‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏£‡∏±‡∏ê‡∏õ‡∏£‡∏∞‡∏®‡∏≤‡∏™‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå 
# ‡∏Ñ‡∏ì‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏•‡∏≤‡∏Å‡∏£‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏ß‡∏∏‡∏í‡∏¥ ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏£‡∏±‡∏ê‡∏õ‡∏£‡∏∞‡∏®‡∏≤‡∏™‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ

# ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏£‡∏±‡∏ê‡∏õ‡∏£‡∏∞‡∏®‡∏≤‡∏™‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ö‡∏±‡∏ì‡∏ë‡∏¥‡∏ï‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤‡∏£‡∏±‡∏ê‡∏õ‡∏£‡∏∞‡∏®‡∏≤‡∏™‡∏ô‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå