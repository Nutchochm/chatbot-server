from bs4 import BeautifulSoup
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PDFPlumberLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from rank_bm25 import BM25Okapi
from transformers import AutoTokenizer, AutoModelForCausalLM
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu
import os
import torch
import requests
import hashlib
import pickle


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
FOLDER = "pdf"
#EMBEDDING_MODEL = "model/paraphrase-multilingual-MiniLM-L12-v2"
EMBEDDING_MODEL = "model_trained/wangchanberta"
VECTOR_STORE_PATH = "vector_store"
METADATA_PATH = VECTOR_STORE_PATH + "index.pkl"
"""
web_urls = [
    "https://www.tru.ac.th/",
    "https://www.google.com/",
    "https://th.wikipedia.org/wiki/%E0%B8%A1%E0%B8%AB%E0%B8%B2%E0%B8%A7%E0%B8%B4%E0%B8%97%E0%B8%A2%E0%B8%B2%E0%B8%A5%E0%B8%B1%E0%B8%A2%E0%B8%A3%E0%B8%B2%E0%B8%8A%E0%B8%A0%E0%B8%B1%E0%B8%8F%E0%B9%80%E0%B8%97%E0%B8%9E%E0%B8%AA%E0%B8%95%E0%B8%A3%E0%B8%B5"
]"""

def get_file_hash(file_path):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì hash ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á"""
    hasher = hashlib.md5()
    with open(file_path, "rb") as f:
        hasher.update(f.read())
    return hasher.hexdigest()

def load_existing_metadata(METADATA_PATH):
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡πâ‡∏ß"""
    if os.path.exists(METADATA_PATH):
        with open(METADATA_PATH, "rb") as f:
            return pickle.load(f)

    return {}

def save_metadata(metadata, METADATA_PATH):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
    with open(METADATA_PATH, "wb") as f:
        pickle.dump(metadata, f)

def create_vector_store(folder_path, web_urls=None):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PDF + ‡πÄ‡∏ß‡πá‡∏ö ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata"""
    documents = []
    metadata_dict = {}  # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• metadata
    
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            pdf_path = os.path.join(folder_path, filename)
            print(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î: {pdf_path}")
            
            loader = PDFPlumberLoader(pdf_path)
            pdf_docs = loader.load()
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° Metadata
            for doc in pdf_docs:
                doc.metadata = {"filename": filename}
            
            documents.extend(pdf_docs)

    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ({len(documents)} ‡πÑ‡∏ü‡∏•‡πå)")

    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö
    if web_urls:
        for url in web_urls:
            print(f"üåê ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö: {url}")
            web_doc = fetch_web_data(url)
            if web_doc:
                documents.append(web_doc)
                print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {url} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
            else:
                print(f"‚ö†Ô∏è ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {url} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ embedding model
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunk ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    docs = text_splitter.split_documents(documents)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metadata
    for idx, doc in enumerate(docs):
        metadata_dict[idx] = doc.metadata

    print(f"üìë ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå: {len(docs)}")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS vector database
    vector_db = FAISS.from_documents(docs, embedding_model)
    vector_db.save_local(VECTOR_STORE_PATH)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Metadata
    save_metadata(metadata_dict, METADATA_PATH)
    print("‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!")

def update_vector_store(PDF_FOLDER, embedding_model):
    """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï FAISS Vector Store ‡πÇ‡∏î‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà ‡∏û‡∏£‡πâ‡∏≠‡∏° Metadata"""
    # ‡πÇ‡∏´‡∏•‡∏î metadata ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
    metadata = load_existing_metadata(METADATA_PATH)

    new_docs = []
    new_metadata = {}

    for filename in os.listdir(PDF_FOLDER):
        if filename.lower().endswith(".pdf"):
            file_path = os.path.join(PDF_FOLDER, filename)
            file_hash = get_file_hash(file_path)

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
            if filename not in metadata or metadata[filename] != file_hash:
                print(f"üìå ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: {filename}")

                # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å PDF
                loader = PDFPlumberLoader(file_path)
                documents = loader.load()

                # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunk ‡∏û‡∏£‡πâ‡∏≠‡∏° metadata
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
                chunked_docs = text_splitter.split_documents(documents)

                for doc in chunked_docs:
                    doc.metadata = {"filename": filename}  # ‡πÉ‡∏™‡πà metadata
                    new_docs.append(doc)
                    new_metadata[len(metadata) + len(new_metadata)] = doc.metadata  # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Metadata ‡πÉ‡∏´‡∏°‡πà

                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å hash ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
                metadata[filename] = file_hash

    if new_docs:
        print("üìå ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï FAISS ‡πÇ‡∏î‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà...")

        # ‡πÇ‡∏´‡∏•‡∏î FAISS ‡πÄ‡∏Å‡πà‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
        if os.path.exists(os.path.join(VECTOR_STORE_PATH, "index.faiss")):
            vector_db = FAISS.load_local(VECTOR_STORE_PATH, embedding_model)
        else:
            vector_db = FAISS.from_documents([], embedding_model)

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤ vector store
        vector_db.add_documents(new_docs)
        vector_db.save_local(VECTOR_STORE_PATH)

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Metadata
        metadata.update(new_metadata)
        save_metadata(metadata, METADATA_PATH)

        print("‚úÖ ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    else:
        print("‚ö° ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï FAISS")

def search_from_vector_store(query, top_k=3):
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

    # ‡πÇ‡∏´‡∏•‡∏î FAISS
    vector_db = FAISS.load_local(VECTOR_STORE_PATH, embedding_model, allow_dangerous_deserialization=True)

    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
    query_vector = embedding_model.embed_query(query)

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
    results = vector_db.similarity_search_by_vector(query_vector, k=top_k)

    return results


def fetch_web_data(url):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÄ‡∏û‡∏à"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    
    # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö
    text = soup.get_text(separator="\n", strip=True)
    
    return Document(page_content=text, metadata={"source": url})

def search_query(query, k=3):
    # ‡πÇ‡∏´‡∏•‡∏î FAISS vector database
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vector_db = FAISS.load_local(VECTOR_STORE_PATH, embedding_model, allow_dangerous_deserialization=True)

    # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    retrieved_docs = vector_db.similarity_search(query, k=k)
    retrieved_text = "\n".join([doc.page_content for doc in retrieved_docs])
    print("retrieved_docs", retrieved_docs)
    return retrieved_text


def loadtyphoon2(modelconfig):
    load_path = os.path.join("model_trained", modelconfig)
    #load_path = "scb10x/llama3.2-typhoon2-1b-instruct"
    # Reload the saved quantized model
    typhoon2_model = AutoModelForCausalLM.from_pretrained(
        load_path,
        device_map="auto",
        local_files_only=True
    )
    typhoon2_tokenizer = AutoTokenizer.from_pretrained(load_path, local_files_only=True)
    return typhoon2_model, typhoon2_tokenizer

def typhoon2chat(query, typhoon2_model, typhoon2_tokenizer):
    messages = [
        {"role": "system", "content": "You are a male AI assistant named Typhoon created by SCB 10X to be helpful, harmless, and honest. Typhoon is happy to help with analysis, question answering, math, coding, creative writing, teaching, role-play, general discussion, and all sorts of other tasks. Typhoon responds directly to all human messages without unnecessary affirmations or filler phrases like ‚ÄúCertainly!‚Äù, ‚ÄúOf course!‚Äù, ‚ÄúAbsolutely!‚Äù, ‚ÄúGreat!‚Äù, ‚ÄúSure!‚Äù, etc. Specifically, Typhoon avoids starting responses with the word ‚ÄúCertainly‚Äù in any way. Typhoon follows this information in all languages, and always responds to the user in the language they use or request. Typhoon is now being connected with a human. Write in fluid, conversational prose, Show genuine interest in understanding requests, Express appropriate emotions and empathy. Also showing information in term that is easy to understand and visualized."},
        {"role": "user", "content": query},
    ]

    input_ids = typhoon2_tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    terminators = [
        typhoon2_tokenizer.eos_token_id,
        typhoon2_tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    ## Typhoon 1b need low temperature to inference.
    outputs = typhoon2_model.generate(
        input_ids,
        max_new_tokens=1024,
        eos_token_id=terminators,
        do_sample=True,
        temperature=0.2,
        top_p=0.8,
    )

    response = outputs[0][input_ids.shape[-1]:]
    #print("Token decoded",typhoon2_tokenizer.decode(response, skip_special_tokens=True))
    return typhoon2_tokenizer.decode(response, skip_special_tokens=True)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å (‡∏£‡∏±‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå)
if not os.path.exists(VECTOR_STORE_PATH):
    create_vector_store(FOLDER)

#prompts = f"""
#   {retrieved_text}
#   Q:
#   {query}
#   A:
#"""

"""
query = "‡∏≠‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° + ‡πÄ‡∏â‡∏•‡∏¢ ‡∏Ç‡∏≠‡∏á‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏£‡∏≤‡∏ä‡∏†‡∏±‡∏è‡πÄ‡∏ó‡∏û‡∏™‡∏ï‡∏£‡∏µ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö"

#retrieved_text = search_query(query, k=3)
retrieved_text = search_from_vector_store(query=query, top_k=3)

print("‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ö:\n", retrieved_text)



modelconfig = "local_typhoon2_1b"
typhoon2_model, typhoon2_tokenizer = loadtyphoon2(modelconfig)
response = typhoon2chat(prompts, typhoon2_model, typhoon2_tokenizer)

print(f"typhoon2 response: {response}")
"""